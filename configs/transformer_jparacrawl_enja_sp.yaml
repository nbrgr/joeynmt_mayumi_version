name: "jparacrawl-enja-sp-transformer"

data:
  task: "MT"
  train: "/scratch5t/ohta/jparacrawl/train"
  dev: "/scratch5t/ohta/iwslt17/tst2016"
  test: "/scratch5t/ohta/iwslt17/tst2017"
  src:
    lang: "en"
    level: "bpe"
    tokenizer: "sentencepiece"
    lowercase: False
    remove_punctuation: False
    max_length: 256
    voc_min_freq: 2
    voc_limit: 32000
    voc_file: "/scratch5t/ohta/jparacrawl/spm.en.32000.vocab.tsv"
    spm:
      model_file: "/scratch5t/ohta/jparacrawl/spm.en.32000.model"  # sentence piece model path
      enable_sampling: True         # whether to enable BPE dropout
  trg:
    lang: "ja"
    level: "bpe"
    tokenizer: "sentencepiece"
    lowercase: False
    remove_punctuation: False
    max_length: 512
    voc_min_freq: 2
    voc_limit: 32000
    voc_file: "/scratch5t/ohta/jparacrawl/spm.ja.32000.ja.tsv"
    spm:
      model_file: "/scratch5t/ohta/jparacrawl/spm.ja.32000.ja.model"  # sentence piece model path
      enable_sampling: True         # whether to enable BPE dropout

testing:
  beam_size: 6
  alpha: 1.0
  sacrebleu:
    remove_whitespace: False
    tokenize: "ja-mecab"

training:
  load_model: "/scratch12t/ohta/models/jparacrawl_enja/avg5.ckpt"
  random_seed: 42
  optimizer: "adam"
  normalization: "tokens"
  adam_betas: [0.9, 0.98]
  scheduling: "warmupinversesquareroot"
  loss: "crossentropy"
  learning_rate: 0.001
  learning_rate_min: 1.0e-09
  learning_rate_warmup: 4000
  clip_grad_norm: 1.0
  weight_decay: 0.0
  label_smoothing: 0.1
  batch_multiplier: 8
  batch_size: 8196 # 4096 per device
  batch_type: "token"
  early_stopping_metric: "eval_metric"
  epochs: 10
  updates: 100000
  validation_freq: 500
  logging_freq: 100
  eval_metrics: "bleu"
  model_dir: "/scratch12t/ohta/models/jparacrawl_enja_sp32000"
  overwrite: False
  shuffle: True
  use_cuda: True
  fp16: True
  max_output_length: 100
  print_valid_sents: [2000, 2001, 2002, 2003, 2004]
  keep_best_ckpts: 5
  num_workers: 0

model:
  initializer: "xavier"
  embed_initializer: "xavier"
  embed_init_gain: 1.0
  init_gain: 1.0
  bias_initializer: "zeros"
  tied_embeddings: False
  tied_softmax: False
  encoder:
    type: "transformer"
    num_layers: 8
    num_heads: 16
    embeddings:
      embedding_dim: 1024
      scale: True
      dropout: 0.
    hidden_size: 1024
    ff_size: 4096
    dropout: 0.3
  decoder:
    type: "transformer"
    num_layers: 6
    num_heads: 16
    embeddings:
      embedding_dim: 1024
      scale: True
      dropout: 0.
    hidden_size: 1024
    ff_size: 4096
    dropout: 0.3
